{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x8cH2rRlfU7i"
   },
   "source": [
    "#### CSCE 670 :: Information Storage & Retrieval :: Texas A&M University :: Spring 2023\n",
    "\n",
    "\n",
    "# Homework 2: Implicit Recommender Systems\n",
    "\n",
    "### 100 points [9% of your final grade]\n",
    "\n",
    "### Due: March 9 (Thursday) by 11:59pm\n",
    "\n",
    "*Goals of this homework:* The objective of this homework is to get you familiar with the pipelines of implicit recommendation, turn the theories of collaborative filtering, matrix factorization, and bayesian personalized ranking (BPR) into practice, and compare the results with the naive non-personalized recommendation to see how personalized collaborative filtering algorithms, matrix factorization model, and the BPR model provide better recommendations for the ranking task (with implicit feedback).\n",
    "\n",
    "*Submission instructions (eCampus):* To submit your homework, rename this notebook as `UIN_hw1.ipynb`. For example, my homework submission would be something like `555001234_hw1.ipynb`. Submit this notebook via eCampus (look for the homework 2 assignment there). Your notebook should be completely self-contained, with the results visible in the notebook. We should not have to run any code from the command line, nor should we have to run your code within the notebook (though we reserve the right to do so). So please run all the cells for us, and then submit.\n",
    "\n",
    "*Late submission policy:* For this homework, you may use as many late days as you like (up to the 5 total allotted to you).\n",
    "\n",
    "*Collaboration policy:* You are expected to complete each homework independently. Your solution should be written by you without the direct aid or help of anyone else. However, we believe that collaboration and team work are important for facilitating learning, so we encourage you to discuss problems and general problem approaches (but not actual solutions) with your classmates. You may post on Piazza, search StackOverflow, etc. But if you do get help in this way, you must inform us by **filling out the Collaboration Declarations at the bottom of this notebook**. \n",
    "\n",
    "*Example: I found helpful code on stackoverflow at https://stackoverflow.com/questions/11764539/writing-fizzbuzz that helped me solve Problem 2.*\n",
    "\n",
    "The basic rule is that no student should explicitly share a solution with another student (and thereby circumvent the basic learning process), but it is okay to share general approaches, directions, and so on. If you feel like you have an issue that needs clarification, feel free to contact either me or the TA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sgS43jLbL77U"
   },
   "source": [
    "# Part 1. Recommendations with Implicit Feedback (20 points)\n",
    "\n",
    "For this part, we're going to build a simple **non-personalized** implicit recommendation algorithm. Since feedback like user clicks, purchases, and views is much more widespread than explicit ratings, implicit recommenders offer great opportunities for far-reaching impact. \n",
    "\n",
    "Concretely, the task of implicit recommendation is to recommend items to users based on implicit signals from users, i.e., we only know what items a user is interested in, but have no idea what items the user dislikes. So for this case, the dataset we could use for this implicit recommendation experiment only contains binary data with 1 representing that the user likes the item, and with 0 representing that we don't know the user's preference towards the item. Because of this, we cannot use the same evaluation method as explicit recommendation. Instead, we need to evaluate the implicit recommendation quality by a ranking task.\n",
    "\n",
    "For this part, we will:\n",
    "* load and process the MovieLens 1M dataset,\n",
    "* transfer the explicit dataset to implicit one,\n",
    "* build a non-personalized implicit recommender, \n",
    "* and evaluate your recommender."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_e5ak01BL77T"
   },
   "source": [
    "To start out, we need to prepare the data. We will use the MovieLens 1M data from https://grouplens.org/datasets/movielens/1m/ in this homework. Lucky for you, we are providing the file containing the ratings -- ratings.dat  -- so all you need to do is load the ratings.dat file in the notebook as a DataFrame variable using the Pandas library. The code to do this has been provided in the next cell, but you need to run it. The resulting data variables are: train_mat is the numpy array variable for training data of size (#users, #items) with non-zero entries representing user-item ratings, and zero entries representing unknown user-item ratings; and test_mat is the numpy array variable for testing data of size (#users, #items)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "ZaocWZ94MD-e"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "# set the seed value\n",
    "np.random.seed(0)\n",
    "\n",
    "data_df = pd.read_csv('./ratings.dat', sep='::', names=[\"UserID\", \"MovieID\", \"Rating\", \"Timestamp\"], engine='python')\n",
    "\n",
    "# First, generate dictionaries for mapping old id to new id for users and movies\n",
    "unique_MovieID = data_df['MovieID'].unique()\n",
    "unique_UserID = data_df['UserID'].unique()\n",
    "j = 0\n",
    "user_old2new_id_dict = dict()\n",
    "for u in unique_UserID:\n",
    "    user_old2new_id_dict[u] = j\n",
    "    j += 1\n",
    "j = 0\n",
    "movie_old2new_id_dict = dict()\n",
    "for i in unique_MovieID:\n",
    "    movie_old2new_id_dict[i] = j\n",
    "    j += 1\n",
    "    \n",
    "# Then, use the generated dictionaries to reindex UserID and MovieID in the data_df\n",
    "user_list = data_df['UserID'].values\n",
    "movie_list = data_df['MovieID'].values\n",
    "for j in range(len(data_df)):\n",
    "    user_list[j] = user_old2new_id_dict[user_list[j]]\n",
    "    movie_list[j] = movie_old2new_id_dict[movie_list[j]]\n",
    "data_df['UserID'] = user_list\n",
    "data_df['movieID'] = movie_list\n",
    "\n",
    "# generate train_df with 70% samples and test_df with 30% samples, and there should have no overlap between them.\n",
    "train_index = np.random.random(len(data_df)) <= 0.7\n",
    "train_df = data_df[train_index]\n",
    "test_df = data_df[~train_index]\n",
    "\n",
    "# generate train_mat and test_mat\n",
    "num_user = len(data_df['UserID'].unique())\n",
    "num_movie = len(data_df['MovieID'].unique())\n",
    "\n",
    "train_mat = coo_matrix((train_df['Rating'].values, (train_df['UserID'].values, train_df['MovieID'].values)), shape=(num_user, num_movie)).astype(float).toarray()\n",
    "test_mat = coo_matrix((test_df['Rating'].values, (test_df['UserID'].values, test_df['MovieID'].values)), shape=(num_user, num_movie)).astype(float).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C-sEqkSHL77V"
   },
   "source": [
    "It is very easy to transfer the explicit datasets you already generated to implicit ones: here, you will consider the watching behavior as the implicit signal showing that the user is interested in a movie. Thus, you can use the same train_df and test_df for implicit recommendation experiment with 'Rating' column ignored. And for train_mat and test_mat, you need to make all ratings to be value 1 and keep 0 entries the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Av77NlOLL77V"
   },
   "outputs": [],
   "source": [
    "# turn the explicit ratings to implicit feedback data\n",
    "train_mat = (train_mat > 0).astype(float)\n",
    "test_mat = (test_mat > 0).astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mvun1WmYL77V"
   },
   "source": [
    "## Part 1a: Build the non-personalized recommender (10 points)\n",
    "\n",
    "In this part, you need to build a non-personalized recommendation model to provide a ranked list of 50 movies as the recommendation for each user. The model is very simple: for each user, the recommendation list is to rank the unwatched movies by their **popularity**, where the popularity is the number of implicit feedback each movie gets. In this case, although it is non-personalized recommender, the recommendation results may be different for users because the unwatched movies are different across users.\n",
    "\n",
    "In the next cell, write your code to generate the ranked lists of movies by the popularity based recommendation algorithm for every user, store the result in a numpy array of size (#user, 50), entry (u, k) represents the movie id that is ranked at position k in the recommendation list to user u. Print out the top-5 recommended movies and their popularity for the first user (with id 0).\n",
    "\n",
    "Hint: the popularity can only be calculated from train_mat, you cannot use test_mat here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "l9rcPRfhL77V"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-5 for first user:\n",
      "Movie: 104; popularity score: 2423.0\n",
      "Movie: 124; popularity score: 2086.0\n",
      "Movie: 64; popularity score: 2064.0\n",
      "Movie: 113; popularity score: 1862.0\n",
      "Movie: 97; popularity score: 1845.0\n"
     ]
    }
   ],
   "source": [
    "# # Generate a ranked list of movies by the popularity based recommendation algorithm. And print out the id and popularity of the top5 movies for the first user.\n",
    "# # Your Code Here...\n",
    "\n",
    "popularity = train_mat.sum(axis=0)\n",
    "\n",
    "sorted_movies = np.argsort(popularity)[: : -1]\n",
    "\n",
    "num_users, num_items = train_mat.shape\n",
    "np_recommendations = np.zeros((num_users, 50), dtype=int)\n",
    "for u in range(num_users):\n",
    "    unwatched_movies = np.where(train_mat[u, :] == 0)[0]\n",
    "    sorted_unwatched_movies = sorted_movies[np.isin(sorted_movies, unwatched_movies)]\n",
    "    np_recommendations[u, :] = sorted_unwatched_movies[:50]\n",
    "\n",
    "top_5_popularity = popularity[top_5_movies]\n",
    "top_5_movies = np_recommendations[0, :5]\n",
    "print(\"Top-5 for first user:\")\n",
    "for i in range(0,5):\n",
    "    print(f\"Movie: {top_5_movies[i]}; popularity score: {top_5_popularity[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[104, 124,  64, ..., 206, 315,  58],\n",
       "       [ 44,  97,  22, ...,  27, 168,  19],\n",
       "       [ 97,  48, 132, ...,  58, 280,  27],\n",
       "       ...,\n",
       "       [104,  44, 124, ..., 210,  78, 213],\n",
       "       [104, 124, 113, ...,  67, 547, 206],\n",
       "       [ 44,  64, 113, ..., 538, 145,  33]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WwzW8K6gL77V"
   },
   "source": [
    "## Part 1b: Evaluate the non-personalized recommender (10 points)\n",
    "\n",
    "In this part, you need to evaluate your non-personalized recommendation by the held-out testing dataset test_mat for each user. For the implicit recommendation, two typical metrics are recall@k and precision@k. Here, you need to write the code to calculate recall@k and  precision@k for k=5, 20, 50 for each user, i.e., six metrics for every user. And please print out the average over all users for these six metrics.\n",
    "\n",
    "Hint: if a user does not have any testing samples in test_mat, do not include this user in the final averaged metric.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "f3I34QYzL77V"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall@5\t[0.037859],\t||\t recall@20\t[0.108581],\t||\t recall@50\t[0.193603]\n",
      "precision@5\t[0.272980],\t||\t precision@20\t[0.210364],\t||\t precision@50\t[0.157795]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate recall@k and precision@k with k=5, 20, 50 and print out the average over all users for these 9 metrics.\n",
    "# Your Code Here...\n",
    "recommendation = np_recommendations\n",
    "\n",
    "recalls = np.zeros(3)\n",
    "precisions = np.zeros(3)\n",
    "user_count = 0.\n",
    "\n",
    "user_test_like = []\n",
    "for u in range(num_user):\n",
    "    user_test_like.append(np.where(test_mat[u, :] > 0)[0])\n",
    "\n",
    "for u in range(num_user):\n",
    "        \n",
    "    test_like = user_test_like[u]\n",
    "    test_like_num = len(test_like)\n",
    "    if test_like_num == 0:\n",
    "        continue\n",
    "    rec = recommendation[u, :]\n",
    "    hits = np.zeros(3)\n",
    "    for k in range(50):\n",
    "        if rec[k] in test_like:\n",
    "            if k < 50:\n",
    "                hits[2] += 1\n",
    "                if k < 20:\n",
    "                    hits[1] += 1\n",
    "                    if k < 5:\n",
    "                        hits[0] += 1\n",
    "    recalls[0] += (hits[0] / test_like_num)\n",
    "    recalls[1] += (hits[1] / test_like_num)\n",
    "    recalls[2] += (hits[2] / test_like_num)\n",
    "    precisions[0] += (hits[0] / 5.)\n",
    "    precisions[1] += (hits[1] / 20.)\n",
    "    precisions[2] += (hits[2] / 50.)\n",
    "    user_count += 1\n",
    "\n",
    "recalls /= user_count\n",
    "precisions /= user_count\n",
    "\n",
    "print('recall@5\\t[%.6f],\\t||\\t recall@20\\t[%.6f],\\t||\\t recall@50\\t[%.6f]' % (recalls[0], recalls[1], recalls[2]))\n",
    "print('precision@5\\t[%.6f],\\t||\\t precision@20\\t[%.6f],\\t||\\t precision@50\\t[%.6f]' % (precisions[0], precisions[1], precisions[2]))\n",
    "print('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_np = recalls\n",
    "pre_np = precisions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qxAu2Gi_fU7m"
   },
   "source": [
    "# Part 2. Build the Collaborative Filtering Model for Implicit Feedback (25 points)\n",
    "\n",
    "In this part, we will need to build **personalized** models instead of non-personalized models as in Part 1. We study how collaborative filtering algorithms work for recommendations with implicit feedback. The overall pipeline is the same as in Part 1. But now you need to implement a user-user collaborative filtering algorithm for recommendation. You will also evaluate your personalized models to compare them with the non-personalized one in Part 1.\n",
    "\n",
    "In this part, you will use the same MovieLens 1M dataset, and:\n",
    "\n",
    "* write the code to implement a user-user collaborative filtering algorithm,\n",
    "* and evaluate your recommender."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rs9uZydYfU7k"
   },
   "source": [
    "First, we need to load and preprocess the experiment dataset. We still use the MovieLens 1M data from https://grouplens.org/datasets/movielens/1m/ in this part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f4Y-6bhZfU7m"
   },
   "source": [
    "Then, you need to write code to implement a user-user collaborative filtering algorithm with **cosine similarity**. Although we have not discussed in class how to use collaborative filtering for implicit feedback, it is quite straightforward. \n",
    "\n",
    "* We first need to calculate the cosine similarity between users based on the binary feedback vectors of users; \n",
    "* Then, for a specific user $u$, we predict a preference vector for the user $u$ by weighted averaging the binary feedback vectors of N users who are most similar to $u$;\n",
    "* And last, rank the movies based on the predicted preference vector of the user $u$ as recommendations. \n",
    "\n",
    "The predicted preference score from user $u$ to movie $i$ can be calculated as: $p_{u,i}=\\frac{\\sum_{u^\\prime\\in N}s(u,u^\\prime)r_{u^\\prime,i}}{\\sum_{u^\\prime\\in N}|s(u,u^\\prime)|}$, where $s(u,u^\\prime)$ is the cosine similarity, and we set the size of $N$ as 10.\n",
    "\n",
    "In the next cell, write your code to generate the ranked lists of movies by this user-user collaborative filtering recommendation algorithm for every user, store the result in a numpy array of size (#user, 50), where entry (u, k) represents the movie id that is ranked at position k in the recommendation list to user u. \n",
    "\n",
    "* Hint: for a user $u$, the movies user $u$ liked in the train_mat should be excluded in the top 50 recommendation list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "Dfh815eYfU7m"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 207,   10,  390, ..., 1314,  389,  733],\n",
       "       [  92,  171,  189, ..., 1228,  719,  217],\n",
       "       [  97,   40,  381, ...,  643,  171,  644],\n",
       "       ...,\n",
       "       [  44,  486,  104, ..., 1297,  184,  623],\n",
       "       [1465,  726, 1623, ..., 1106, 2313, 1633],\n",
       "       [ 499,  136,    0, ..., 2066, 1529, 1528]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# calculate cosine similarity between users based on binary feedback\n",
    "user_sim = cosine_similarity(train_mat)\n",
    "\n",
    "# set the number of similar users to consider for each user\n",
    "N = 10\n",
    "\n",
    "# array to store cos recommend\n",
    "cos_recommend = np.zeros((num_users, 50), dtype=int)\n",
    "\n",
    "#now we need to take top 10 similar users for each user and recommend movies for this user\n",
    "for u in range(num_users):\n",
    "    u_sim = user_sim[u]\n",
    "    u_sim[u] = -np.inf\n",
    "    top_10_uids = np.argsort(u_sim)[::-1][:N]\n",
    "    top_10_usim = u_sim[top_10_uids]\n",
    "    \n",
    "    denominator = sum(top_10_usim)\n",
    "\n",
    "    \n",
    "    # Find the movies that the user has not watched\n",
    "    unwatched_movies = np.where(train_mat[u, :] == 0)[0]\n",
    "    \n",
    "    movie_pref_u_unwatched = np.zeros(len(unwatched_movies))\n",
    "    #iterator k\n",
    "    k = 0\n",
    "    \n",
    "    #now iterate movies and find p for each\n",
    "    for m in unwatched_movies:\n",
    "        # Extract the m-th column for the rows in top_10_uids\n",
    "        sim_users_rating = train_mat[top_10_uids, m]\n",
    "        #dot between top_10_usim and sim_users_rating\n",
    "        numerator = np.dot(sim_users_rating, top_10_usim)\n",
    "        movie_pref_u_unwatched[k] = numerator/denominator\n",
    "        k += 1\n",
    "        \n",
    "    #sort these unwatched movies by their pref score\n",
    "    sort_uwatched_pref_ind = np.argsort(movie_pref_u_unwatched)[::-1]\n",
    "    \n",
    "    #store the movie id's of these \n",
    "    rec_movies = unwatched_movies[sort_uwatched_pref_ind]\n",
    "        \n",
    "    # Store the IDs of the top 50 recommended movies for user u\n",
    "    cos_recommend[u] = rec_movies[:50]\n",
    "cos_recommend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v73XLfN5fU7m"
   },
   "source": [
    "Last, you need to evaluate your user-user collaborative filtering algorithm by the held-out testing dataset test_mat for each user. Here, we use the same metrics recall@k and precision@k we used in Part 2c of Module 1. So you can use the same code here to calculate recall@k and precision@k for k=5, 20, 50 for each user, i.e., six metrics for every user. And please print out the average over all users for these six metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "-UI-iWnLfU7m"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall@5\t[0.070497],\t||\t recall@20\t[0.191733],\t||\t recall@50\t[0.321965]\n",
      "precision@5\t[0.378146],\t||\t precision@20\t[0.290911],\t||\t precision@50\t[0.221732]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate recall@k, precision@k with k=5, 20, 50 and print out the average over all users for these 6 metrics.\n",
    "# Your Code Here...\n",
    "\n",
    "recommendation = cos_recommend\n",
    "\n",
    "recalls = np.zeros(3)\n",
    "precisions = np.zeros(3)\n",
    "user_count = 0.\n",
    "\n",
    "user_test_like = []\n",
    "for u in range(num_user):\n",
    "    user_test_like.append(np.where(test_mat[u, :] > 0)[0])\n",
    "\n",
    "for u in range(num_user):\n",
    "        \n",
    "    test_like = user_test_like[u]\n",
    "    test_like_num = len(test_like)\n",
    "    if test_like_num == 0:\n",
    "        continue\n",
    "    rec = recommendation[u, :]\n",
    "    hits = np.zeros(3)\n",
    "    for k in range(50):\n",
    "        if rec[k] in test_like:\n",
    "            if k < 50:\n",
    "                hits[2] += 1\n",
    "                if k < 20:\n",
    "                    hits[1] += 1\n",
    "                    if k < 5:\n",
    "                        hits[0] += 1\n",
    "    recalls[0] += (hits[0] / test_like_num)\n",
    "    recalls[1] += (hits[1] / test_like_num)\n",
    "    recalls[2] += (hits[2] / test_like_num)\n",
    "    precisions[0] += (hits[0] / 5.)\n",
    "    precisions[1] += (hits[1] / 20.)\n",
    "    precisions[2] += (hits[2] / 50.)\n",
    "    user_count += 1\n",
    "\n",
    "recalls /= user_count\n",
    "precisions /= user_count\n",
    "\n",
    "print('recall@5\\t[%.6f],\\t||\\t recall@20\\t[%.6f],\\t||\\t recall@50\\t[%.6f]' % (recalls[0], recalls[1], recalls[2]))\n",
    "print('precision@5\\t[%.6f],\\t||\\t precision@20\\t[%.6f],\\t||\\t precision@50\\t[%.6f]' % (precisions[0], precisions[1], precisions[2]))\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_cf = recalls\n",
    "pre_cf = precisions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bh07jVftmBAY"
   },
   "source": [
    "**Question (5 points):** do you observe a better result compared with models implemented in the previous part? What's reason do you think that brings this improvement?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "6en1Xna8mBAZ"
   },
   "source": [
    "Write your answer here:\n",
    "\n",
    "The results for non-personalized recommender:\n",
    "\n",
    "recall@5\t[0.037859],\t||\t recall@20\t[0.108581],\t||\t recall@50\t[0.193603]\n",
    "precision@5\t[0.272980],\t||\t precision@20\t[0.210364],\t||\t precision@50\t[0.157795]\n",
    " \n",
    " \n",
    "The results for Collaborative Filtering Model:\n",
    "\n",
    "recall@5\t[0.070497],\t||\t recall@20\t[0.191733],\t||\t recall@50\t[0.321965]\n",
    "precision@5\t[0.378146],\t||\t precision@20\t[0.290911],\t||\t precision@50\t[0.221732]\n",
    "\n",
    "\n",
    "\n",
    "As we can observe, non-personalized recommender has less recall (recall@5, recall@20 ,recall@50) and precision (precision@5,\tprecision@20,\tprecision@50) \n",
    "\n",
    "Therefore I find Collaborative Filtering Model better among the both. But, there can be no clear winner because the optimal winner depends on the use case and other priorities. But, as I find Collaborative Filtering Mode better in all aspects I consider it better among them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Tl2tFlibnAY"
   },
   "source": [
    "# Part 3. Build the Matrix Factorization Model for Implicit Feedback (25 points)\n",
    "\n",
    "Now we turn to study how matrix factorization algorithms works for recommendations with implicit feedback. The overall pipeline is the same as in Part 1 and Part 2. But now you need to implement a matrix factorization algorithm for recommendation. You will also compare your matrix factorization model to the non-personalized one in Part 1 and collaborative filtering one in Part 2.\n",
    "\n",
    "In this part, you will use the same MovieLens 1M dataset, and:\n",
    "* write the code to implement a matrix factorization algorithm,\n",
    "* and evaluate your recommender."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XTLaMD2CbnAS"
   },
   "source": [
    "First, let's recap the matrix factorization model introduced in class. The MF model can be mathematically represented as: \n",
    "\n",
    "<center>$\\underset{\\mathbf{P},\\mathbf{Q}}{\\text{min}}\\,\\,L=\\sum_{(u,i)\\in\\mathcal{O}}(\\mathbf{P}_u\\cdot\\mathbf{Q}^\\top_i-r_{u,i})^2+\\lambda(\\lVert\\mathbf{P}\\rVert^2_{\\text{F}}+\\lVert\\mathbf{Q}\\rVert^2_{\\text{F}})$,</center>\n",
    "    \n",
    "where $\\mathbf{P}$ is the user latent factor matrix of size (#user, #latent); $\\mathbf{Q}$ is the movie latent factor matrix of size (#movie, #latent); $\\mathcal{O}$ is a user-movie pair set containing all user-movie pairs having ratings in train_mat; $r_{u,i}$ represents the rating for user u and movie i; $\\lambda(\\lVert\\mathbf{P}\\rVert^2_{\\text{F}}+\\lVert\\mathbf{Q}\\rVert^2_{\\text{F}})$ is the regularization term to overcome overfitting problem, $\\lambda$ is the regularization weight (a hyper-parameter manually set by developer, i.e., you), and $\\lVert\\mathbf{P}\\rVert^2_{\\text{F}}=\\sum_{x}\\sum_{y}(\\mathbf{P}_{x,y})^2$, $\\lVert\\mathbf{Q}\\rVert^2_{\\text{F}}=\\sum_{x}\\sum_{y}(\\mathbf{Q}_{x,y})^2$. Such an L function is called the **loss function** for the matrix factorization model. The goal of training an MF model is to find appropriate $\\mathbf{P}$ and $\\mathbf{Q}$ to minimize the loss L."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Po4byk_1bnAZ"
   },
   "source": [
    "Although we have not discussed in class how to use matrix factorization algorithm for implicit feedback, it is quite straightforward. The main idea is the same as the MF model for explicit ratings. But instead of predicting explicit ratings, here the MF is to predict binary ratings based on which movies are ranked for users.\n",
    "\n",
    "The only challenge now is that in an implicit feedback dataset, there is only positive signal (i.e., '1' in the train_mat) without negative signal. Hence, to let our MF work for this implicit feedback, a simple but powerful method -- random negative sampling -- is adopted. The main idea is that in each training epoch, we randomly sample user-movie pairs without positive feedback in train_mat (user-movie pairs with '0' in train_mat) to be negative feedback, and mix them with positive feedback as the training data to train the MF model. The **negative sampling method is already provided**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i-YYpBikbnAZ"
   },
   "source": [
    "In the Next cell, you need to complete the MF_implicit class. There are five functions in this class: \n",
    "\n",
    "* 'init' (**provided**) is to initialize the variables the MF class needs, which takes 5 inputs: train_mat, test_mat, latent, lr, and reg. 'train_mat' and 'test_mat' are the corresponfing training and testing matrices we have. 'latent' represents the latent dimension we set for the MF model. 'lr' represents the learning rate, i.e., the update step in each optimization iteration, default is 0.01. 'reg' represents the regularization weight, i.e., the $\\lambda$ in the MF formulation. \n",
    "\n",
    "* 'negative_sampling' (**provided**) is to do the random negative sampling to generate negative signals for training the MF. It returns a list of users and a list of movies as the training samples, mixing positive and negative user-movie pairs.\n",
    "\n",
    "* 'test' (**provided**) is to evaluate the trained MF on test_mat and print out recall@k and precision@k. \n",
    "\n",
    "* 'predict' (**need to be completed**) is to generate the ranked lists of movies by the trained MF for every user, store the ranking result (named 'recommendation') in a numpy array of size (#user, 50), where entry (u, k) represents the movie id that is ranked at position k in the recommendation list to user u. Return the 'recommendation' variable. \n",
    "\n",
    "* 'train' (**need to be completed**) is to train the MF model. There is only one input to this function: an int variable 'epoch' to indicate how many epochs for training the model. The main logic of this function is the same as the one you already implemented in Part 1a. The main body of this function should be a loop for 'epoch' iterations. In each iteration, following the algorithm to update the MF model:\n",
    "\n",
    "        1. Call 'negative_sampling()' to generate a list of users and a list of movies mixing positive and negative user-movie pairs as training samples.\n",
    "        2. Randomly shuffle training user-movie pairs  (i.e., user-movie pairs from step 1)\n",
    "        2. Have an inner loop to iterate each user-movie pair:\n",
    "                a. given a user-movie pair (u,i), update the user latent factor and movie latent factor by gradient decsent:    \n",
    "<center>$\\mathbf{P}_u=\\mathbf{P}_u-\\gamma [2(\\mathbf{P}_u\\cdot\\mathbf{Q}_i^\\top-r_{u,i})\\cdot\\mathbf{Q}_i+2\\lambda\\mathbf{P}_u]$</center>    \n",
    "<center>$\\mathbf{Q}_i=\\mathbf{Q}_i-\\gamma [2(\\mathbf{P}_u\\cdot\\mathbf{Q}_i^\\top-r_{u,i})\\cdot\\mathbf{P}_u+2\\lambda\\mathbf{Q}_i]$</center>    \n",
    "<center>where $\\mathbf{P}_u$ and $\\mathbf{Q}_i$ are row vectors of size (1, #latent), $\\gamma$ is learning rate (default is 0.01), $\\lambda$ is regularization weight, and $r_{u,i}$ now takes binary value.</center>\n",
    "        \n",
    "        3. After iterating over all user-movie pairs, call 'test()' to evaluate the current MF model.\n",
    "\n",
    "**Note: you are not supposed to delete or modify the provided code.**\n",
    "\n",
    "**Note: for this part, it is necessary to read and understand the provided code, because you will need to call the provided functions in your code.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "NCQi55FTbnAa"
   },
   "outputs": [],
   "source": [
    "class MF_implicit:\n",
    "    def __init__(self, train_mat, test_mat, latent=5, lr=0.01, reg=0.01):\n",
    "        self.train_mat = train_mat  # the training rating matrix of size (#user, #movie)\n",
    "        self.test_mat = test_mat  # the training rating matrix of size (#user, #movie)\n",
    "        \n",
    "        self.latent = latent  # the latent dimension\n",
    "        self.lr = lr  # learning rate\n",
    "        self.reg = reg  # regularization weight, i.e., the lambda in the objective function\n",
    "        \n",
    "        self.num_user, self.num_movie = train_mat.shape\n",
    "        \n",
    "        self.sample_user, self.sample_movie = self.train_mat.nonzero()  # get the user-movie paris having ratings in train_mat\n",
    "        self.num_sample = len(self.sample_user)  # the number of user-movie pairs having ratings in train_mat\n",
    "\n",
    "        self.user_test_like = []\n",
    "        for u in range(self.num_user):\n",
    "            self.user_test_like.append(np.where(self.test_mat[u, :] > 0)[0])\n",
    "\n",
    "        self.P = np.random.random((self.num_user, self.latent))  # latent factors for users, size (#user, self.latent), randomly initialized\n",
    "        self.Q = np.random.random((self.num_movie, self.latent))  # latent factors for users, size (#movie, self.latent), randomly initialized\n",
    "        \n",
    "    def negative_sampling(self):\n",
    "        negative_movie = np.random.choice(np.arange(self.num_movie), size=(len(self.sample_user)), replace=True)\n",
    "        true_negative = self.train_mat[self.sample_user, negative_movie] == 0\n",
    "        negative_user = self.sample_user[true_negative]\n",
    "        negative_movie = negative_movie[true_negative]\n",
    "        return np.concatenate([self.sample_user, negative_user]), np.concatenate([self.sample_movie, negative_movie])\n",
    "\n",
    "    def train(self, epoch=20):\n",
    "        \"\"\"\n",
    "        Goal: Write your code to train your matrix factorization model for epoch iterations in this function\n",
    "        Input: epoch -- the number of training epoch \n",
    "        \"\"\"\n",
    "        for ep in range(epoch):\n",
    "            \"\"\" \n",
    "            Write your code here to implement the training process for one epoch, \n",
    "            at the end of each epoch, run self.test() to evaluate current version of MF.\n",
    "            \"\"\"\n",
    "            user_list, movie_list = self.negative_sampling()\n",
    "            \n",
    "            #randomly shuffle\n",
    "            \n",
    "            # Combine the two lists using zip()\n",
    "            combined_list = list(zip(user_list, movie_list))\n",
    "\n",
    "            # Shuffle the combined list randomly\n",
    "            random.shuffle(combined_list)\n",
    "\n",
    "            # Unpack the shuffled list into separate user and movie lists using zip()\n",
    "            shuffled_user_list, shuffled_movie_list = zip(*combined_list)\n",
    "            \n",
    "            \n",
    "            \n",
    "            # update latent matrices P and Q\n",
    "            for u, i in zip(shuffled_user_list, shuffled_movie_list):\n",
    "                err = 2 * (np.dot(self.P[u], self.Q[i].T) - (self.train_mat[u, i]))\n",
    "                p_u = self.P[u].copy()\n",
    "                q_i = self.Q[i].copy()\n",
    "                self.P[u] = p_u - (self.lr * ((err * q_i) + (2 * self.reg * p_u)))\n",
    "                self.Q[i] = q_i - (self.lr * ((err * p_u) + (2 * self.reg * q_i)))\n",
    "\n",
    "            print(\"epoch\")\n",
    "            print(ep)\n",
    "            pre, rec = self.test()\n",
    "            \n",
    "        return pre, rec            \n",
    "        \"\"\"\n",
    "        End of your code for this function\n",
    "        \"\"\"\n",
    "            \n",
    "    def predict(self):\n",
    "        \"\"\"\n",
    "        Write your code here to implement the prediction function, which generates the ranked lists of movies \n",
    "        by the trained MF for every user, store the result (named 'recommendation') in a numpy array of size (#user, 50), where entry (u, k) \n",
    "        represents the movie id that is ranked at position k in the recommendation list to user u. Return the 'recommendation' variable. \n",
    "        \"\"\"\n",
    "        recommendation = np.zeros((self.num_user, 50))\n",
    "        for u in range(0,self.num_user):\n",
    "            # predict the ranking score for unrated movies\n",
    "            ranking = np.dot(self.P[u], self.Q.T)\n",
    "            # exclude rated movies\n",
    "            ranking[self.train_mat[u].nonzero()] = -np.inf\n",
    "            # top 50 recommended movies\n",
    "            recommendation[u] = np.argsort(ranking)[::-1][:50]\n",
    "        return recommendation\n",
    "    \n",
    "    def test(self):\n",
    "        recommendation = self.predict()\n",
    "\n",
    "        recalls = np.zeros(3)\n",
    "        precisions = np.zeros(3)\n",
    "        user_count = 0.\n",
    "\n",
    "        for u in range(self.num_user):\n",
    "            test_like = self.user_test_like[u]\n",
    "            test_like_num = len(test_like)\n",
    "            if test_like_num == 0:\n",
    "                continue\n",
    "            rec = recommendation[u, :]\n",
    "            hits = np.zeros(3)\n",
    "            for k in range(50):\n",
    "                if rec[k] in test_like:\n",
    "                    if k < 50:\n",
    "                        hits[2] += 1\n",
    "                        if k < 20:\n",
    "                            hits[1] += 1\n",
    "                            if k < 5:\n",
    "                                hits[0] += 1\n",
    "            recalls[0] += (hits[0] / test_like_num)\n",
    "            recalls[1] += (hits[1] / test_like_num)\n",
    "            recalls[2] += (hits[2] / test_like_num)\n",
    "            precisions[0] += (hits[0] / 5.)\n",
    "            precisions[1] += (hits[1] / 20.)\n",
    "            precisions[2] += (hits[2] / 50.)\n",
    "            user_count += 1\n",
    "\n",
    "        recalls /= user_count\n",
    "        precisions /= user_count\n",
    "\n",
    "        print('recall@5\\t[%.6f],\\t||\\t recall@20\\t[%.6f],\\t||\\t recall@50\\t[%.6f]' % (recalls[0], recalls[1], recalls[2]))\n",
    "        print('precision@5\\t[%.6f],\\t||\\t precision@20\\t[%.6f],\\t||\\t precision@50\\t[%.6f]' % (precisions[0], precisions[1], precisions[2]))\n",
    "        print('')\n",
    "        \n",
    "        return precisions, recalls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GK9lahvXbnAa"
   },
   "source": [
    "Now, run the next cell to build and train your implemented MF model for implicit feedback. The expected time used for training one epoch is 20s to 2 min."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "l1K6sLgEbnAa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch\n",
      "0\n",
      "recall@5\t[0.013178],\t||\t recall@20\t[0.056748],\t||\t recall@50\t[0.134744]\n",
      "precision@5\t[0.144172],\t||\t precision@20\t[0.142425],\t||\t precision@50\t[0.127778]\n",
      "\n",
      "epoch\n",
      "1\n",
      "recall@5\t[0.023075],\t||\t recall@20\t[0.082760],\t||\t recall@50\t[0.167991]\n",
      "precision@5\t[0.202914],\t||\t precision@20\t[0.178990],\t||\t precision@50\t[0.145361]\n",
      "\n",
      "epoch\n",
      "2\n",
      "recall@5\t[0.026367],\t||\t recall@20\t[0.087157],\t||\t recall@50\t[0.172315]\n",
      "precision@5\t[0.222086],\t||\t precision@20\t[0.184851],\t||\t precision@50\t[0.148738]\n",
      "\n",
      "epoch\n",
      "3\n",
      "recall@5\t[0.031098],\t||\t recall@20\t[0.094336],\t||\t recall@50\t[0.174677]\n",
      "precision@5\t[0.242086],\t||\t precision@20\t[0.191921],\t||\t precision@50\t[0.148487]\n",
      "\n",
      "epoch\n",
      "4\n",
      "recall@5\t[0.028618],\t||\t recall@20\t[0.092695],\t||\t recall@50\t[0.180527]\n",
      "precision@5\t[0.236457],\t||\t precision@20\t[0.195290],\t||\t precision@50\t[0.155699]\n",
      "\n",
      "epoch\n",
      "5\n",
      "recall@5\t[0.029047],\t||\t recall@20\t[0.090238],\t||\t recall@50\t[0.178050]\n",
      "precision@5\t[0.257616],\t||\t precision@20\t[0.202641],\t||\t precision@50\t[0.162689]\n",
      "\n",
      "epoch\n",
      "6\n",
      "recall@5\t[0.030731],\t||\t recall@20\t[0.096635],\t||\t recall@50\t[0.185425]\n",
      "precision@5\t[0.268113],\t||\t precision@20\t[0.216126],\t||\t precision@50\t[0.173371]\n",
      "\n",
      "epoch\n",
      "7\n",
      "recall@5\t[0.029231],\t||\t recall@20\t[0.096830],\t||\t recall@50\t[0.193738]\n",
      "precision@5\t[0.261457],\t||\t precision@20\t[0.221002],\t||\t precision@50\t[0.181672]\n",
      "\n",
      "epoch\n",
      "8\n",
      "recall@5\t[0.032017],\t||\t recall@20\t[0.102464],\t||\t recall@50\t[0.202943]\n",
      "precision@5\t[0.275530],\t||\t precision@20\t[0.229346],\t||\t precision@50\t[0.188076]\n",
      "\n",
      "epoch\n",
      "9\n",
      "recall@5\t[0.031688],\t||\t recall@20\t[0.104600],\t||\t recall@50\t[0.210686]\n",
      "precision@5\t[0.269801],\t||\t precision@20\t[0.230430],\t||\t precision@50\t[0.190689]\n",
      "\n",
      "epoch\n",
      "10\n",
      "recall@5\t[0.034265],\t||\t recall@20\t[0.111460],\t||\t recall@50\t[0.219102]\n",
      "precision@5\t[0.283444],\t||\t precision@20\t[0.238220],\t||\t precision@50\t[0.195003]\n",
      "\n",
      "epoch\n",
      "11\n",
      "recall@5\t[0.035236],\t||\t recall@20\t[0.113202],\t||\t recall@50\t[0.221198]\n",
      "precision@5\t[0.283675],\t||\t precision@20\t[0.238286],\t||\t precision@50\t[0.194503]\n",
      "\n",
      "epoch\n",
      "12\n",
      "recall@5\t[0.035452],\t||\t recall@20\t[0.115809],\t||\t recall@50\t[0.225697]\n",
      "precision@5\t[0.284404],\t||\t precision@20\t[0.240257],\t||\t precision@50\t[0.196858]\n",
      "\n",
      "epoch\n",
      "13\n",
      "recall@5\t[0.036237],\t||\t recall@20\t[0.115693],\t||\t recall@50\t[0.229594]\n",
      "precision@5\t[0.284139],\t||\t precision@20\t[0.239255],\t||\t precision@50\t[0.197450]\n",
      "\n",
      "epoch\n",
      "14\n",
      "recall@5\t[0.036294],\t||\t recall@20\t[0.118495],\t||\t recall@50\t[0.233967]\n",
      "precision@5\t[0.282748],\t||\t precision@20\t[0.239685],\t||\t precision@50\t[0.197871]\n",
      "\n",
      "epoch\n",
      "15\n",
      "recall@5\t[0.038617],\t||\t recall@20\t[0.122407],\t||\t recall@50\t[0.235728]\n",
      "precision@5\t[0.293311],\t||\t precision@20\t[0.245124],\t||\t precision@50\t[0.199699]\n",
      "\n",
      "epoch\n",
      "16\n",
      "recall@5\t[0.039982],\t||\t recall@20\t[0.123535],\t||\t recall@50\t[0.240224]\n",
      "precision@5\t[0.295762],\t||\t precision@20\t[0.244677],\t||\t precision@50\t[0.200563]\n",
      "\n",
      "epoch\n",
      "17\n",
      "recall@5\t[0.040113],\t||\t recall@20\t[0.125139],\t||\t recall@50\t[0.244433]\n",
      "precision@5\t[0.298344],\t||\t precision@20\t[0.247359],\t||\t precision@50\t[0.203010]\n",
      "\n",
      "epoch\n",
      "18\n",
      "recall@5\t[0.039342],\t||\t recall@20\t[0.126843],\t||\t recall@50\t[0.244722]\n",
      "precision@5\t[0.293113],\t||\t precision@20\t[0.247533],\t||\t precision@50\t[0.201874]\n",
      "\n",
      "epoch\n",
      "19\n",
      "recall@5\t[0.039313],\t||\t recall@20\t[0.125554],\t||\t recall@50\t[0.245815]\n",
      "precision@5\t[0.291060],\t||\t precision@20\t[0.245944],\t||\t precision@50\t[0.202526]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mf_implicit = MF_implicit(train_mat, test_mat, latent=5, lr=0.01, reg=0.0001)\n",
    "pre_mf,rec_mf = mf_implicit.train(epoch=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall@5\t[0.039313],\t||\t recall@20\t[0.125554],\t||\t recall@50\t[0.245815]\n",
      "precision@5\t[0.291060],\t||\t precision@20\t[0.245944],\t||\t precision@50\t[0.202526]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('recall@5\\t[%.6f],\\t||\\t recall@20\\t[%.6f],\\t||\\t recall@50\\t[%.6f]' % (rec_mf[0], rec_mf[1], rec_mf[2]))\n",
    "print('precision@5\\t[%.6f],\\t||\\t precision@20\\t[%.6f],\\t||\\t precision@50\\t[%.6f]' % (pre_mf[0], pre_mf[1], pre_mf[2]))\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KA_IrnxJl961"
   },
   "source": [
    "**Question (5 points):** do you observe a better result compared with models implemented in previous two parts? What's reason do you think that brings this improvement?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "wxwdsGYel961"
   },
   "source": [
    "Write your answer here:\n",
    "\n",
    "The results for non-personalized recommender:\n",
    "\n",
    "recall@5\t[0.037859],\t||\t recall@20\t[0.108581],\t||\t recall@50\t[0.193603]\n",
    "precision@5\t[0.272980],\t||\t precision@20\t[0.210364],\t||\t precision@50\t[0.157795]\n",
    " \n",
    " \n",
    "The results for Collaborative Filtering Model:\n",
    "\n",
    "recall@5\t[0.070497],\t||\t recall@20\t[0.191733],\t||\t recall@50\t[0.321965]\n",
    "precision@5\t[0.378146],\t||\t precision@20\t[0.290911],\t||\t precision@50\t[0.221732]\n",
    "\n",
    "The results for Matrix Factorization Model:\n",
    "\n",
    "recall@5\t[0.039313],\t||\t recall@20\t[0.125554],\t||\t recall@50\t[0.245815]\n",
    "precision@5\t[0.291060],\t||\t precision@20\t[0.245944],\t||\t precision@50\t[0.202526]\n",
    "\n",
    "\n",
    "We can observe that the  Collaborative Filtering Model is better in all aspects such as recall (recall@5, recall@20 ,recall@50) and precision (precision@5,\tprecision@20,\tprecision@50) when compared to the other two. There has been an improvement in precision and recall in Matrix Factorization Model when compared to non-personalized recommender, but again it could not beat the Collaborative Filtering Model. In order to choose a recommender, we need to look at the use case and effectiveness parameters, but here it looks like Collaborative Filtering Model is better in all aspects.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4PW9KO1gMD-e"
   },
   "source": [
    "# Part 4: Build the Bayesian Personalized Ranking (BPR) Model with Implicit Feedback (20 points)\n",
    "\n",
    "In this first part, you will need to build a **Bayesian Personalized Ranking (BPR)** model. You will also evaluate your BPR model to compare with the non-personalized one in Part 1, neighborhood-based collaborative filtering model in Part 2, and matrix factorization model in Part 3. \n",
    "\n",
    "For this part, we will:\n",
    "\n",
    "* build a BPR model,\n",
    "* and evaluate your recommender."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t-hWSeW5MD-e"
   },
   "source": [
    "We still use the MovieLens 1M data from https://grouplens.org/datasets/movielens/1m/ in this part, and use the same loading and preprocessing process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MO69amdAMD-k"
   },
   "source": [
    "First, let's implement the Bayesian Personalized Ranking model introduced in class. The BPR model can be mathematically represented as: \n",
    "\n",
    "<center>$\\underset{\\mathbf{P},\\mathbf{Q}}{\\text{max}}\\,\\,L=\\sum_{(u,i,j)\\in\\mathcal{O}}\\sigma(\\mathbf{P}_u\\cdot\\mathbf{Q}^\\top_i-\\mathbf{P}_u\\cdot\\mathbf{Q}^\\top_j)+\\lambda(\\lVert\\mathbf{P}\\rVert^2_{\\text{F}}+\\lVert\\mathbf{Q}\\rVert^2_{\\text{F}})$,</center>\n",
    "    \n",
    "where $\\mathbf{P}$ is the user latent factor matrix of size (#user, #latent); $\\mathbf{Q}$ is the movie latent factor matrix of size (#movie, #latent); $\\sigma(\\cdot)$ is the Sigmoid function; $\\mathcal{O}$ is a (user, positive movie, negative movie) tuple set, and each tuple $(u,i,j)$ is a training sample with user $u$ and a posotive movie $i$ with value 1 in train_mat and a negative movie $j$ with value 0 in train_mat; $\\lambda(\\lVert\\mathbf{P}\\rVert^2_{\\text{F}}+\\lVert\\mathbf{Q}\\rVert^2_{\\text{F}})$ is the regularization term to overcome overfitting problem, $\\lambda$ is the regularization weight (a hyper-parameter manually set by the developer, i.e., you), and $\\lVert\\mathbf{P}\\rVert^2_{\\text{F}}=\\sum_{x}\\sum_{y}(\\mathbf{P}_{x,y})^2$, $\\lVert\\mathbf{Q}\\rVert^2_{\\text{F}}=\\sum_{x}\\sum_{y}(\\mathbf{Q}_{x,y})^2$. Such an L function is called the **loss function** for the matrix factorization model. The goal of training a BPR model is to find appropriate $\\mathbf{P}$ and $\\mathbf{Q}$ to **maximize** the loss L."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2kpemLFvMD-l"
   },
   "source": [
    "To implement such a BPR model, here we will write a Python class for the model. Similar to the implicit_MF model you have already implemented in Part 3, there are five functions in this BPR class: \n",
    "\n",
    "* The 'init' function (**provided**) is to initialize the variables the BPR class needs, which takes 5 inputs: train_mat, test_mat, latent, lr, and reg. 'train_mat' and 'test_mat' are the corresponfing training and testing matrices we have. 'latent' represents the latent dimension we set for the BPR model. 'lr' represents the learning rate, i.e., the update step in each optimization iteration, default is 0.01. 'reg' represents the regularization weight, i.e., the $\\lambda$ in the BPR formulation.\n",
    "\n",
    "* 'negative_sampling' (**provided**) is to do the random negative sampling to generate negative signals for training the BPR. It returns a list of users, a list of positive movies of these users, and a list of negative movies of these user. These three lists form the training set $\\mathcal{O}$.\n",
    "\n",
    "* 'test' (**provided**) is to evaluate the trained MF on test_mat and print out recall@k and precision@k. \n",
    "\n",
    "* 'predict' (**provided**) is to generates the ranked lists of movies by the trained MF for every user, store the ranking result (named 'recommendation') in a numpy array of size (#user, 50), where entry (u, k) represents the movie id that is ranked at position k in the recommendation list to user u. Return the 'recommendation' variable. \n",
    "\n",
    "* 'train' (**need to be completed**) is to train the BPR model. There is only one input to this function: an int variable 'epoch' to indicate how many epochs for training the model. The main process of one training epoch is:\n",
    "\n",
    "        1. Call 'negative_sampling()' to generate training samples: a list of users, a list of positive movies of these users, and a list of negative movies of these users.\n",
    "        2. Randomly shuffle training samples from step 1.\n",
    "        3. Have an inner loop to iterate each (user, positive movie, negative movie) tuple in the shuffled training samples:\n",
    "                a. given the current training tuple -- a user u, a positive movie i, and a negative movie j, update the user latent factor and movie latent factor by gradient decsent:\n",
    "<center>$\\mathbf{P}_u=\\mathbf{P}_u - \\gamma[-\\frac{e^{-\\widehat{x}}}{1 + e^{-\\widehat{x}}} (\\mathbf{Q}_i - \\mathbf{Q}_j) + \\lambda\\mathbf{P}_u]$</center>\n",
    "<center>$\\mathbf{Q}_i=\\mathbf{Q}_i - \\gamma[-\\frac{e^{-\\widehat{x}}}{1 + e^{-\\widehat{x}}} \\mathbf{P}_U + \\lambda\\mathbf{Q}_i]$</center>\n",
    "<center>$\\mathbf{Q}_j=\\mathbf{Q}_j - \\gamma[\\frac{e^{-\\widehat{x}}}{1 + e^{-\\widehat{x}}} \\mathbf{P}_U + \\lambda\\mathbf{Q}_j]$</center>   \n",
    "<center>where $\\mathbf{P}_u$, $\\mathbf{Q}_i$, and $\\mathbf{Q}_j$ are of size (1, #latent), $\\gamma$ is learning rate (default is 0.01), $\\lambda$ is regularization weight, and $\\widehat{x}=\\mathbf{P}_u\\cdot\\mathbf{Q}^\\top_i-\\mathbf{P}_u\\cdot\\mathbf{Q}^\\top_j$.</center>\n",
    "        \n",
    "        3. After iterate all training samples, call 'test()' to evaluate the current BPR model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lYQmt-3wMD-l"
   },
   "source": [
    "In the next cell, you will need to fill in the 'train' function based on the description above. \n",
    "\n",
    "*Hint that similar to the training process of MF in Part 3, in each update iteration, you need to use the old P and Q from last update iteration to update P and Q in current update iteration.\n",
    "\n",
    "**NOTE that you should not delete or modify the provided code.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "R7oKJUR9MD-l"
   },
   "outputs": [],
   "source": [
    "class BPR:\n",
    "    def __init__(self, train_mat, test_mat, latent=5, lr=0.01, reg=0.01):\n",
    "        self.train_mat = train_mat  # the training rating matrix of size (#user, #movie)\n",
    "        self.test_mat = test_mat  # the training rating matrix of size (#user, #movie)\n",
    "        \n",
    "        self.latent = latent  # the latent dimension\n",
    "        self.lr = lr  # learning rate\n",
    "        self.reg = reg  # regularization weight, i.e., the lambda in the objective function\n",
    "        \n",
    "        self.num_user, self.num_movie = train_mat.shape\n",
    "        \n",
    "        self.positive_user, self.positive_movie = self.train_mat.nonzero()  # get the user-movie paris having ratings in train_mat\n",
    "\n",
    "        self.user_test_like = []\n",
    "        for u in range(self.num_user):\n",
    "            self.user_test_like.append(np.where(self.test_mat[u, :] > 0)[0])\n",
    "\n",
    "        self.P = np.random.random((self.num_user, self.latent))  # latent factors for users, size (#user, self.latent), randomly initialized\n",
    "        self.Q = np.random.random((self.num_movie, self.latent))  # latent factors for users, size (#movie, self.latent), randomly initialized\n",
    "        \n",
    "    def negative_sampling(self): \n",
    "        # do the negative sampling for each of the positive user-movie pair. Here we set negative sampling rate as 2, \n",
    "        # i.e., for each positive user-movie pair, randomly sample two negative movies. \n",
    "        # This function returns final training data: a user list, a positive movie list, and a negative movie list\n",
    "        sample_user = np.tile(self.positive_user, 2)\n",
    "        sample_pos_movie = np.tile(self.positive_movie, 2)\n",
    "        sample_neg_movie = np.random.choice(np.arange(self.num_movie), size=(len(sample_user)), replace=True)\n",
    "        true_negative = self.train_mat[sample_user, sample_neg_movie] == 0\n",
    "        return sample_user[true_negative], sample_pos_movie[true_negative], sample_neg_movie[true_negative]\n",
    "\n",
    "    def train(self, epoch=20):\n",
    "        \"\"\"\n",
    "        Goal: Write your code to train your matrix factorization model for epoch iterations in this function\n",
    "        Input: epoch -- the number of training epoch \n",
    "        \"\"\"\n",
    "        for ep in range(epoch):\n",
    "            \"\"\" \n",
    "            Write your code here to implement the training process for one epoch, \n",
    "            at the end of each epoch, run self.test() to evaluate current version of BPR.\n",
    "            \"\"\"\n",
    "            users, pos_items, neg_items = self.negative_sampling()\n",
    "            samples = list(zip(users, pos_items, neg_items))\n",
    "            np.random.shuffle(samples)\n",
    "            # Unpack the shuffled list into separate user and movie lists using zip()\n",
    "            shuffled_user_list, shuffled_movie_pos,shuffled_movie_neg = zip(*samples)\n",
    "            for u, i, j in zip(shuffled_user_list, shuffled_movie_pos,shuffled_movie_neg):\n",
    "                p_u = self.P[u].copy()\n",
    "                q_i = self.Q[i].copy()\n",
    "                q_j = self.Q[j].copy()\n",
    "\n",
    "#                 x = np.dot(self.P[u], self.Q[i] - self.Q[j])\n",
    "                x = np.dot(self.P[u], self.Q[i].T) - np.dot(self.P[u], self.Q[j].T)\n",
    "                err = np.exp(-x) / (1.0 + np.exp(-x))\n",
    "                self.P[u] = p_u - ( self.lr * ( (-1 * err * (q_i - q_j)) + ( self.reg * p_u)  )   )\n",
    "                self.Q[i] = q_i - ( self.lr * ( (-1 * err * (p_u)) + ( self.reg * q_i)  )   )\n",
    "                self.Q[j] = q_j - ( self.lr * ( (err * (p_u)) + ( self.reg * q_j)  )   )\n",
    "            \n",
    "            print(\"epoch\")\n",
    "            print(ep)\n",
    "            \n",
    "            pre, rec = self.test()\n",
    "            \n",
    "        return pre, rec\n",
    "        \"\"\"\n",
    "        End of your code for this function\n",
    "        \"\"\"\n",
    "            \n",
    "    def predict(self):\n",
    "        # The prediction function, which generates the ranked lists of movies \n",
    "        # by the trained BPR for every user, store the result (named 'recommendation') in a numpy array of size (#user, 50), where entry (u, k) \n",
    "        # represents the movie id that is ranked at position k in the recommendation list to user u. Return the 'recommendation' variable. \n",
    "        prediction_mat = np.matmul(self.P, self.Q.T)\n",
    "        recommendation = []\n",
    "        for u in range(self.num_user):\n",
    "            scores = prediction_mat[u]\n",
    "            train_like = np.where(train_mat[u, :] > 0)[0]\n",
    "            scores[train_like] = -9999\n",
    "            top50_iid = np.argpartition(scores, -50)[-50:]\n",
    "            top50_iid = top50_iid[np.argsort(scores[top50_iid])[-1::-1]]\n",
    "            recommendation.append(top50_iid)\n",
    "        recommendation = np.array(recommendation)\n",
    "        return recommendation\n",
    "    \n",
    "    def test(self):\n",
    "        recommendation = self.predict()\n",
    "\n",
    "        recalls = np.zeros(3)\n",
    "        precisions = np.zeros(3)\n",
    "        user_count = 0.\n",
    "\n",
    "        for u in range(self.num_user):\n",
    "            test_like = self.user_test_like[u]\n",
    "            test_like_num = len(test_like)\n",
    "            if test_like_num == 0:\n",
    "                continue\n",
    "            rec = recommendation[u, :]\n",
    "            hits = np.zeros(3)\n",
    "            for k in range(50):\n",
    "                if rec[k] in test_like:\n",
    "                    if k < 50:\n",
    "                        hits[2] += 1\n",
    "                        if k < 20:\n",
    "                            hits[1] += 1\n",
    "                            if k < 5:\n",
    "                                hits[0] += 1\n",
    "            recalls[0] += (hits[0] / test_like_num)\n",
    "            recalls[1] += (hits[1] / test_like_num)\n",
    "            recalls[2] += (hits[2] / test_like_num)\n",
    "            precisions[0] += (hits[0] / 5.)\n",
    "            precisions[1] += (hits[1] / 20.)\n",
    "            precisions[2] += (hits[2] / 50.)\n",
    "            user_count += 1\n",
    "\n",
    "        recalls /= user_count\n",
    "        precisions /= user_count\n",
    "\n",
    "        print('recall@5\\t[%.6f],\\t||\\t recall@20\\t[%.6f],\\t||\\t recall@50\\t[%.6f]' % (recalls[0], recalls[1], recalls[2]))\n",
    "        print('precision@5\\t[%.6f],\\t||\\t precision@20\\t[%.6f],\\t||\\t precision@50\\t[%.6f]' % (precisions[0], precisions[1], precisions[2]))\n",
    "        print('')\n",
    "        \n",
    "        return precisions, recalls\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uvJRalhwMD-r"
   },
   "source": [
    "Now, let's train a BPR model based on your implementation. The code is provided, you just need to execute the next cell. The expectations are: \n",
    "\n",
    "* first, the code can be successfully executed without error; \n",
    "* and second, the recall@k and precision@k on **test_mat** of each training epoch should be printed out for all 20 epochs.\n",
    "\n",
    "\n",
    "* Hint: the expected time used for training is around 1 minute to 5 minutes per training epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "T-q9CtdDMD-s"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch\n",
      "0\n",
      "recall@5\t[0.033957],\t||\t recall@20\t[0.103853],\t||\t recall@50\t[0.188034]\n",
      "precision@5\t[0.260993],\t||\t precision@20\t[0.205911],\t||\t precision@50\t[0.155434]\n",
      "\n",
      "epoch\n",
      "1\n",
      "recall@5\t[0.033963],\t||\t recall@20\t[0.105015],\t||\t recall@50\t[0.187602]\n",
      "precision@5\t[0.260265],\t||\t precision@20\t[0.207094],\t||\t precision@50\t[0.154679]\n",
      "\n",
      "epoch\n",
      "2\n",
      "recall@5\t[0.035242],\t||\t recall@20\t[0.106503],\t||\t recall@50\t[0.188057]\n",
      "precision@5\t[0.263377],\t||\t precision@20\t[0.207864],\t||\t precision@50\t[0.155603]\n",
      "\n",
      "epoch\n",
      "3\n",
      "recall@5\t[0.034252],\t||\t recall@20\t[0.106050],\t||\t recall@50\t[0.189113]\n",
      "precision@5\t[0.262649],\t||\t precision@20\t[0.207988],\t||\t precision@50\t[0.155748]\n",
      "\n",
      "epoch\n",
      "4\n",
      "recall@5\t[0.035456],\t||\t recall@20\t[0.105614],\t||\t recall@50\t[0.189202]\n",
      "precision@5\t[0.264470],\t||\t precision@20\t[0.207558],\t||\t precision@50\t[0.155974]\n",
      "\n",
      "epoch\n",
      "5\n",
      "recall@5\t[0.034924],\t||\t recall@20\t[0.106960],\t||\t recall@50\t[0.191807]\n",
      "precision@5\t[0.262881],\t||\t precision@20\t[0.208816],\t||\t precision@50\t[0.158189]\n",
      "\n",
      "epoch\n",
      "6\n",
      "recall@5\t[0.035750],\t||\t recall@20\t[0.108101],\t||\t recall@50\t[0.190584]\n",
      "precision@5\t[0.269570],\t||\t precision@20\t[0.213063],\t||\t precision@50\t[0.160275]\n",
      "\n",
      "epoch\n",
      "7\n",
      "recall@5\t[0.035914],\t||\t recall@20\t[0.109951],\t||\t recall@50\t[0.194580]\n",
      "precision@5\t[0.276391],\t||\t precision@20\t[0.219222],\t||\t precision@50\t[0.165735]\n",
      "\n",
      "epoch\n",
      "8\n",
      "recall@5\t[0.037469],\t||\t recall@20\t[0.110537],\t||\t recall@50\t[0.199873]\n",
      "precision@5\t[0.288742],\t||\t precision@20\t[0.223957],\t||\t precision@50\t[0.171424]\n",
      "\n",
      "epoch\n",
      "9\n",
      "recall@5\t[0.039143],\t||\t recall@20\t[0.113700],\t||\t recall@50\t[0.206021]\n",
      "precision@5\t[0.300563],\t||\t precision@20\t[0.232012],\t||\t precision@50\t[0.178291]\n",
      "\n",
      "epoch\n",
      "10\n",
      "recall@5\t[0.039363],\t||\t recall@20\t[0.116085],\t||\t recall@50\t[0.211097]\n",
      "precision@5\t[0.307881],\t||\t precision@20\t[0.238907],\t||\t precision@50\t[0.183950]\n",
      "\n",
      "epoch\n",
      "11\n",
      "recall@5\t[0.041336],\t||\t recall@20\t[0.119948],\t||\t recall@50\t[0.218641]\n",
      "precision@5\t[0.319470],\t||\t precision@20\t[0.244503],\t||\t precision@50\t[0.189344]\n",
      "\n",
      "epoch\n",
      "12\n",
      "recall@5\t[0.042865],\t||\t recall@20\t[0.122824],\t||\t recall@50\t[0.223943]\n",
      "precision@5\t[0.328212],\t||\t precision@20\t[0.249834],\t||\t precision@50\t[0.193076]\n",
      "\n",
      "epoch\n",
      "13\n",
      "recall@5\t[0.043902],\t||\t recall@20\t[0.125149],\t||\t recall@50\t[0.229455]\n",
      "precision@5\t[0.332715],\t||\t precision@20\t[0.254015],\t||\t precision@50\t[0.196785]\n",
      "\n",
      "epoch\n",
      "14\n",
      "recall@5\t[0.044892],\t||\t recall@20\t[0.128162],\t||\t recall@50\t[0.234454]\n",
      "precision@5\t[0.340629],\t||\t precision@20\t[0.257947],\t||\t precision@50\t[0.199960]\n",
      "\n",
      "epoch\n",
      "15\n",
      "recall@5\t[0.045804],\t||\t recall@20\t[0.129521],\t||\t recall@50\t[0.238072]\n",
      "precision@5\t[0.343311],\t||\t precision@20\t[0.260679],\t||\t precision@50\t[0.202768]\n",
      "\n",
      "epoch\n",
      "16\n",
      "recall@5\t[0.046896],\t||\t recall@20\t[0.131953],\t||\t recall@50\t[0.243101]\n",
      "precision@5\t[0.347086],\t||\t precision@20\t[0.263129],\t||\t precision@50\t[0.204834]\n",
      "\n",
      "epoch\n",
      "17\n",
      "recall@5\t[0.047259],\t||\t recall@20\t[0.134064],\t||\t recall@50\t[0.246349]\n",
      "precision@5\t[0.351887],\t||\t precision@20\t[0.266424],\t||\t precision@50\t[0.207129]\n",
      "\n",
      "epoch\n",
      "18\n",
      "recall@5\t[0.048309],\t||\t recall@20\t[0.135856],\t||\t recall@50\t[0.250055]\n",
      "precision@5\t[0.353113],\t||\t precision@20\t[0.268278],\t||\t precision@50\t[0.208752]\n",
      "\n",
      "epoch\n",
      "19\n",
      "recall@5\t[0.048788],\t||\t recall@20\t[0.137591],\t||\t recall@50\t[0.251897]\n",
      "precision@5\t[0.355629],\t||\t precision@20\t[0.271573],\t||\t precision@50\t[0.210166]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bpr = BPR(train_mat, test_mat, latent=5, lr=0.01, reg=0.001)\n",
    "pre_bpr,rec_bpr = bpr.train(epoch=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall@5\t[0.048788],\t||\t recall@20\t[0.137591],\t||\t recall@50\t[0.251897]\n",
      "precision@5\t[0.355629],\t||\t precision@20\t[0.271573],\t||\t precision@50\t[0.210166]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('recall@5\\t[%.6f],\\t||\\t recall@20\\t[%.6f],\\t||\\t recall@50\\t[%.6f]' % (rec_bpr[0], rec_bpr[1], rec_bpr[2]))\n",
    "print('precision@5\\t[%.6f],\\t||\\t precision@20\\t[%.6f],\\t||\\t precision@50\\t[%.6f]' % (pre_bpr[0], pre_bpr[1], pre_bpr[2]))\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ptxnml0-MD-s"
   },
   "source": [
    "**Question (5 points):** do you observe a better result compared with models implemented in previous three parts? What's reason do you think that brings this improvement?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "Q0M28QNVMD-s"
   },
   "source": [
    "Write your answer here:\n",
    "\n",
    "The results for non-personalized recommender:\n",
    "\n",
    "recall@5\t[0.037859],\t||\t recall@20\t[0.108581],\t||\t recall@50\t[0.193603]\n",
    "precision@5\t[0.272980],\t||\t precision@20\t[0.210364],\t||\t precision@50\t[0.157795]\n",
    " \n",
    " \n",
    "The results for Collaborative Filtering Model:\n",
    "\n",
    "recall@5\t[0.070497],\t||\t recall@20\t[0.191733],\t||\t recall@50\t[0.321965]\n",
    "precision@5\t[0.378146],\t||\t precision@20\t[0.290911],\t||\t precision@50\t[0.221732]\n",
    "\n",
    "The results for Matrix Factorization Model:\n",
    "\n",
    "recall@5\t[0.039313],\t||\t recall@20\t[0.125554],\t||\t recall@50\t[0.245815]\n",
    "precision@5\t[0.291060],\t||\t precision@20\t[0.245944],\t||\t precision@50\t[0.202526]\n",
    "\n",
    "The results for Bayesian Personalized Ranking (BPR):\n",
    "\n",
    "recall@5\t[0.048788],\t||\t recall@20\t[0.137591],\t||\t recall@50\t[0.251897]\n",
    "precision@5\t[0.355629],\t||\t precision@20\t[0.271573],\t||\t precision@50\t[0.210166]\n",
    "\n",
    "\n",
    "Among the 4 models, Collaborative Filtering Model has better precision and recall. There has been an improvement in Bayesian Personalized Ranking (BPR) model's precision and recall when compared to Matrix Factorization Model and non-personalized recommender but it couldn't beat the Collaborative Filtering Model. \n",
    "In order to choose a recommender, we need to look at the use case and effectiveness parameters, but here it looks like Collaborative Filtering Model is better in all aspects.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wKZ8ijNPTXMN"
   },
   "source": [
    "# Part 5. Cool Extension! (10 points)\n",
    "\n",
    "Just like in the first homework, now is your chance to try an interesting extension. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement Hybering recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all the recommendations\n",
    "\n",
    "# non personalised\n",
    "NP_rec = np_recommendations   #(user id ,50 items ids recommended to the user)\n",
    "#user user cf\n",
    "CF_rec = cos_recommend  #(user id ,50 items ids recommended to the user)\n",
    "#MF\n",
    "MF_rec = mf_implicit.predict()  #(user id ,50 items ids recommended to the user)\n",
    "#BPR\n",
    "BPR_rec = bpr.predict()  #(user id ,50 items ids recommended to the user)\n",
    "\n",
    "#Load all the precesion and recall for top 50\n",
    "\n",
    "# precision@k and recall@k values for each recommender\n",
    "precision_k = np.array([pre_np,   # non-personalized\n",
    "                      pre_cf,   # collaborative filtering\n",
    "                      pre_mf,   # matrix factorization\n",
    "                      pre_bpr])  # BPR\n",
    "\n",
    "recall_k = np.array([rec_np,      # non-personalized\n",
    "                   rec_cf,      # collaborative filtering\n",
    "                   rec_mf,      # matrix factorization\n",
    "                   rec_bpr])     # BPR\n",
    "\n",
    "\n",
    "# define weights based on relative performance\n",
    "w = np.zeros(4)\n",
    "w[0] = np.mean(precision_k[0]) + np.mean(recall_k[0])  # non-personalized\n",
    "w[1] = np.mean(precision_k[1]) + np.mean(recall_k[1])  # collaborative filtering\n",
    "w[2] = np.mean(precision_k[2]) + np.mean(recall_k[2])  # matrix factorization\n",
    "w[3] = np.mean(precision_k[3]) + np.mean(recall_k[3])  # BPR\n",
    "\n",
    "w /= np.sum(w)\n",
    "\n",
    "\n",
    "#find combine above items recommended by each recommender and find top 20 new items\n",
    "\n",
    "#find score for each item on the basis of above weights and recommend top 50 scored items\n",
    "\n",
    "#combine all reccommended items for each user\n",
    "new_items = [[] for u in range(num_users)]\n",
    "\n",
    "# Combine the recommendations\n",
    "for u in range(0,num_users):\n",
    "    combined_list = []\n",
    "    for lst in [NP_rec[u] , CF_rec[u] , MF_rec[u] , BPR_rec[u]]:\n",
    "        combined_list.extend(lst)\n",
    "    combined_list = list(set(combined_list))\n",
    "    new_items[u] = combined_list\n",
    "\n",
    "#find score for each movie for a user and add it to recom list\n",
    "new_hybrid_rec = [[] for u in range(num_users)]\n",
    "\n",
    "\n",
    "for u in range(0,num_users):\n",
    "    mv = new_items[u]\n",
    "    movies = [int(num) for num in mv]\n",
    "    score_movies = np.zeros(len(movies))\n",
    "    k = 0\n",
    "    for m in movies:\n",
    "        s = 0\n",
    "        if m in NP_rec[u]:\n",
    "            s += w[0]\n",
    "        if m in CF_rec[u]:\n",
    "            s += w[1]\n",
    "        if m in MF_rec[u]:\n",
    "            s += w[2]\n",
    "        if m in BPR_rec[u]:\n",
    "            s += w[3]\n",
    "            \n",
    "        score_movies[k] = s\n",
    "        k+=1\n",
    "    \n",
    "    score_movies_sorted = np.argsort(score_movies)[::-1]\n",
    "    recommended_movies = [movies[i] for i in score_movies_sorted[:50]]\n",
    "    new_hybrid_rec[u] = recommended_movies\n",
    "        \n",
    "hybrid_rec  = np.array(new_hybrid_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall@5\t[0.058835],\t||\t recall@20\t[0.168515],\t||\t recall@50\t[0.289817]\n",
      "precision@5\t[0.370265],\t||\t precision@20\t[0.300075],\t||\t precision@50\t[0.227742]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# lets test this reccommender\n",
    "\n",
    "# Calculate recall@k and precision@k with k=5, 20, 50 and print out the average over all users for these 9 metrics.\n",
    "# Your Code Here...\n",
    "recommendation = hybrid_rec\n",
    "\n",
    "recalls = np.zeros(3)\n",
    "precisions = np.zeros(3)\n",
    "user_count = 0.\n",
    "\n",
    "user_test_like = []\n",
    "for u in range(num_user):\n",
    "    user_test_like.append(np.where(test_mat[u, :] > 0)[0])\n",
    "\n",
    "for u in range(num_user):\n",
    "        \n",
    "    test_like = user_test_like[u]\n",
    "    test_like_num = len(test_like)\n",
    "    if test_like_num == 0:\n",
    "        continue\n",
    "    rec = recommendation[u, :]\n",
    "    hits = np.zeros(3)\n",
    "    for k in range(50):\n",
    "        if rec[k] in test_like:\n",
    "            if k < 50:\n",
    "                hits[2] += 1\n",
    "                if k < 20:\n",
    "                    hits[1] += 1\n",
    "                    if k < 5:\n",
    "                        hits[0] += 1\n",
    "    recalls[0] += (hits[0] / test_like_num)\n",
    "    recalls[1] += (hits[1] / test_like_num)\n",
    "    recalls[2] += (hits[2] / test_like_num)\n",
    "    precisions[0] += (hits[0] / 5.)\n",
    "    precisions[1] += (hits[1] / 20.)\n",
    "    precisions[2] += (hits[2] / 50.)\n",
    "    user_count += 1\n",
    "\n",
    "recalls /= user_count\n",
    "precisions /= user_count\n",
    "\n",
    "print('recall@5\\t[%.6f],\\t||\\t recall@20\\t[%.6f],\\t||\\t recall@50\\t[%.6f]' % (recalls[0], recalls[1], recalls[2]))\n",
    "print('precision@5\\t[%.6f],\\t||\\t precision@20\\t[%.6f],\\t||\\t precision@50\\t[%.6f]' % (precisions[0], precisions[1], precisions[2]))\n",
    "print('')\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The results for Hybrid model:\n",
    "\n",
    "recall@5\t[0.058835],\t||\t recall@20\t[0.168515],\t||\t recall@50\t[0.289817]\n",
    "precision@5\t[0.370265],\t||\t precision@20\t[0.300075],\t||\t precision@50\t[0.227742]\n",
    "\n",
    "The results for Collaborative Filtering Model:\n",
    "\n",
    "recall@5\t[0.070497],\t||\t recall@20\t[0.191733],\t||\t recall@50\t[0.321965]\n",
    "precision@5\t[0.378146],\t||\t precision@20\t[0.290911],\t||\t precision@50\t[0.221732]\n",
    "\n",
    "\n",
    "The Hybrid model has better precision@20 and precision@50 compared to the Collaborative Filtering Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cWKpnX8DfU7j"
   },
   "source": [
    "# Collaboration Declarations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kg5RckUJfU7j"
   },
   "source": [
    "** You should fill out your collaboration declarations here.**\n",
    "\n",
    "**Reminder:** You are expected to complete each homework independently. Your solution should be written by you without the direct aid or help of anyone else. However, we believe that collaboration and team work are important for facilitating learning, so we encourage you to discuss problems and general problem approaches (but not actual solutions) with your classmates. You may post on Piazza, search StackOverflow, etc. But if you do get help in this way, you must inform us by filling out the Collaboration Declarations at the bottom of this notebook.\n",
    "\n",
    "Example: I found helpful code on stackoverflow at https://stackoverflow.com/questions/11764539/writing-fizzbuzz that helped me solve Problem 2."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
